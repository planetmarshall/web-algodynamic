---
layout: post
title: Visualizing the Prime Ministerial Debates
tags:
- Imaging
- politics
- python
- Software
status: publish
type: post
published: false
meta:
  _syntaxhighlighter_encoded: '1'
  dsq_thread_id: '285113393'
---
<p>[singlepic id=131 w=140 float=right]</p>  <p class="pm_first">For the first time, the main Prime Ministerial candidates for the 2010 UK General Elections, will take part in <a title="Debates page from the BBC" href="http://news.bbc.co.uk/1/hi/uk_politics/election_2010/the_debates/default.stm" target="_blank">three live debates</a>. Since the BBC have kindly made the full transcripts available, I decided to have a go at analyzing the data and creating a <a title="Jump to image gallery" href="#gallery" target="_self">visual representation</a> in the form of word clouds. I am currently working on my own visualization software, but in the meantime these have been done using <a title="Wordle" href="http://www.wordle.net/" target="_blank">Wordle</a>.</p>    <!--more-->    <h3>Preparing the data</h3>  <p>[singlepic id=138 w=140 float=left]The BBC only provides the data in PDF form - to analyze it we need it in text form. Although this is easily done with Acrobat Reader's &quot;Save as Text&quot; function, the output it produces is not really suitable for automatic processing, so some work has to be done by hand. This basically involves making sure each speaker's comments are headed by their name and some kind of special character to split each record ( here I have used '@' ), which took about 15 minutes or so.</p>  <p><a title="First Prime Ministerial debate in raw text form" href="http://bit.ly/9QRrXx" target="_blank">Download </a>the raw text of the first debate.</p>  <p>Having done that, a command line tool such as <a href="http://www.gnu.org/manual/gawk/gawk.html" target="_blank">awk</a> can be used to split the data by speaker. For example, the following command outputs Clegg's comments into a separate file:</p>  
[code lang="bash"]
awk 'BEGIN {RS=&quot;&quot;; FS=&quot;[@]&quot;} $1==&quot;NC&quot; { print $2 }' debate.txt &gt; clegg.txt
[/code]

<h3>Parsing the data</h3>

<p>[singlepic id=137 w=140 float=right]<a title="Python homepage" href="http://www.python.org/" target="_blank">Python</a>'s <a title="The Natural Language Toolkit" href="http://www.nltk.org/" target="_blank">Natural Language Toolkit</a> provides all the functions needed to analyze the text data, such as tokenizing the text by word and even categorizing each word by type, such as proper nouns and prepositions. For example, having extracted Nick Clegg's speech as above and read the file as a string using Python, the following commands parse the input for sentences, and then tokenize each word procucing a complete word list.</p>

[code lang="python"]
from __future__ import division
import nltk, re, pprint

sentences = nltk.sent_tokenize(text)
tokens=[]
for s = sentences:
 tokens.extend(nltk.word_tokenize(s))
words=[t.lower() for t in tokens]
[/code]

<p>We can then categorize each word with a <a title="Wikipdeia page on POS tagging" href="http://en.wikipedia.org/wiki/Part-of-speech_tagging">POS tag</a> and extract a list appropriately, for example, using the word tokens above the following extracts all the nouns</p>

<pre class="brush: py; gutter: false; toolbar: false;"># this operation takes some time to execute
taggedwords=nltk.pos_tag(words)
nouns=[word for (word,tag) in words if t == 'NN']</pre>

<p>Such a list is enough to use with Wordle, however it's straightforward to create a word frequency list for use with other software.</p>

[code lang="python"]
nounfrequencies = nltk.FreqDist(nouns)
[/code]

<h3>Going further</h3>

<p>Word frequency analyses are fairly straightforward, however NLTK is a powerful library and allows for much more detailed and informative analysis based on grammar and sentence structure. It would be interesting to see the results of a more sophisticated approach.
  <br /><a name="gallery"></a></p>

<h3>Gallery of word clouds from the first debate</h3>

<p>[nggallery id=13]</p>
